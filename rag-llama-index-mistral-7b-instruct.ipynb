{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6187202,"sourceType":"datasetVersion","datasetId":3551175},{"sourceId":7552373,"sourceType":"datasetVersion","datasetId":4398733},{"sourceId":7556307,"sourceType":"datasetVersion","datasetId":4400579}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -qU sentence-transformers\n!pip install -qU git+https://github.com/run-llama/llama_index\n!pip install -qU transformers accelerate bitsandbytes\n!pip install -qU chromadb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-05T07:25:32.596831Z","iopub.execute_input":"2024-02-05T07:25:32.597240Z","iopub.status.idle":"2024-02-05T07:26:46.629282Z","shell.execute_reply.started":"2024-02-05T07:25:32.597201Z","shell.execute_reply":"2024-02-05T07:26:46.628079Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Initial pipeline - using text files to do QnA","metadata":{}},{"cell_type":"code","source":"from llama_index.embeddings import HuggingFaceEmbedding, AdapterEmbeddingModel\nfrom llama_index import VectorStoreIndex, SimpleDirectoryReader, SummaryIndex, set_global_tokenizer\nfrom IPython.display import Markdown, display\nfrom llama_index import StorageContext, load_index_from_storage\nfrom llama_index import ServiceContext, set_global_service_context\nimport chromadb\nfrom llama_index.vector_stores import ChromaVectorStore\nfrom llama_index.llms import HuggingFaceLLM, OpenAI\nfrom llama_index.prompts import PromptTemplate\nfrom transformers import AutoTokenizer\nimport torch\nfrom transformers import BitsAndBytesConfig\nfrom llama_index.response.notebook_utils import display_response\nfrom llama_index.finetuning import EmbeddingAdapterFinetuneEngine\nfrom llama_index.embeddings import resolve_embed_model\nimport torch\nfrom llama_index.embeddings.adapter_utils import TwoLayerNN","metadata":{"execution":{"iopub.status.busy":"2024-02-05T07:27:32.444434Z","iopub.execute_input":"2024-02-05T07:27:32.445535Z","iopub.status.idle":"2024-02-05T07:27:32.453192Z","shell.execute_reply.started":"2024-02-05T07:27:32.445492Z","shell.execute_reply":"2024-02-05T07:27:32.452337Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n# documents = SimpleDirectoryReader(\"/kaggle/input/aws-case-studies-and-blogs\").load_data()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T07:28:13.589635Z","iopub.execute_input":"2024-02-05T07:28:13.590028Z","iopub.status.idle":"2024-02-05T07:28:13.594428Z","shell.execute_reply.started":"2024-02-05T07:28:13.589999Z","shell.execute_reply":"2024-02-05T07:28:13.593455Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T07:32:43.379416Z","iopub.execute_input":"2024-02-05T07:32:43.380211Z","iopub.status.idle":"2024-02-05T07:32:43.386283Z","shell.execute_reply.started":"2024-02-05T07:32:43.380174Z","shell.execute_reply":"2024-02-05T07:32:43.385393Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"llm = HuggingFaceLLM(\n    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n    query_wrapper_prompt=PromptTemplate(\"<s>[INST] {query_str} [/INST] </s>\\n\"),\n    context_window=3900,\n    max_new_tokens=256,\n    model_kwargs={\"quantization_config\": quantization_config},\n    # tokenizer_kwargs={},\n    generate_kwargs={\"temperature\": 0.2, \"top_k\": 5, \"top_p\": 0.95},\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T07:32:45.115124Z","iopub.execute_input":"2024-02-05T07:32:45.116094Z","iopub.status.idle":"2024-02-05T07:32:59.812918Z","shell.execute_reply.started":"2024-02-05T07:32:45.116056Z","shell.execute_reply":"2024-02-05T07:32:59.811902Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729d1b2c36074efc9cd4227f908c5a4d"}},"metadata":{}}]},{"cell_type":"code","source":"service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\nvector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\nsummary_index = SummaryIndex.from_documents(documents, service_context=service_context)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:21:20.312436Z","iopub.execute_input":"2024-02-05T06:21:20.313111Z","iopub.status.idle":"2024-02-05T06:21:47.860919Z","shell.execute_reply.started":"2024-02-05T06:21:20.313076Z","shell.execute_reply":"2024-02-05T06:21:47.859913Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec082ae7379148e991f790006e7ef51e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b64d507e67ff41cba08fcfefd080c080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd8e83794d7480ca6d0e2e74f3c8f20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2299edf9f842579102ab00e6856081"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d7764f3473479fb55da8a473045619"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b794f184aa40c588dc391b5acf0047"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Basic querying","metadata":{}},{"cell_type":"code","source":"query_engine = vector_index.as_query_engine(response_mode=\"compact\")\nresponse = query_engine.query(\"How are manufacturing companies in automotive industry and others using Gen AI on cloud?\")\ndisplay_response(response)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:21:47.862247Z","iopub.execute_input":"2024-02-05T06:21:47.862567Z","iopub.status.idle":"2024-02-05T06:22:28.421686Z","shell.execute_reply.started":"2024-02-05T06:21:47.862530Z","shell.execute_reply":"2024-02-05T06:22:28.420763Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\nSetting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n2024-02-05 06:21:54.397245: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-05 06:21:54.397341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-05 06:21:54.666129: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**`Final Response:`** Generative AI is being used by manufacturing companies in the automotive industry and others to transform their businesses and disrupt their industries. The potential of generative AI is incredibly exciting, but companies are still in the early days of using it. The study by IDC titled \"The State of Manufacturing and Generative AI Adoption in Manufacturing Organizations\" revealed that for manufacturers, the top business areas where survey respondents felt generative AI could make the most impact in the next 18 months were in manufacturing (production), product development and design, followed by sales and supply chain.\n\nIn the automotive industry, generative AI is being used to create radical, new product designs, drive unprecedented levels of manufacturing productivity, and optimize supply chain applications. For example, Autodesk, a leader in 3D design, engineering, and entertainment software, offers generative design capabilities in their Fusion 360 software to help product designers create innovative new designs within parameters specified by the user, including materials, manufacturing constraints, safety factors, and other variables.\n\nGenerative AI is also being used in the automotive industry to improve safety, create simulation datasets, explore how a part might be manufactured or machined faster, and bring"},"metadata":{}}]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"# Finetuning EMBEDDING:\n[LlamaIndex source](https://docs.llamaindex.ai/en/stable/examples/finetuning/embeddings/finetune_embedding_adapter.html#finetuning-an-adapter-on-top-of-any-black-box-embedding-model)","metadata":{}},{"cell_type":"markdown","source":"### 1. Data prep","metadata":{}},{"cell_type":"code","source":"import json\nfrom llama_index import SimpleDirectoryReader\nfrom llama_index.node_parser import SentenceSplitter\nfrom llama_index.schema import MetadataMode","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:22:28.422919Z","iopub.execute_input":"2024-02-05T06:22:28.423216Z","iopub.status.idle":"2024-02-05T06:22:28.427677Z","shell.execute_reply.started":"2024-02-05T06:22:28.423190Z","shell.execute_reply":"2024-02-05T06:22:28.426763Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"TRAIN_FILES = \"/kaggle/input/aws-case-study-data-with-splits/data/train/input/\"\nVAL_FILES = \"/kaggle/input/aws-case-study-data-with-splits/data/val/input/\"\n\nTRAIN_CORPUS_FPATH = \"./data_samples/train_corpus.json\"\nVAL_CORPUS_FPATH = \"./data_samples/val_corpus.json\"","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:25:08.068996Z","iopub.execute_input":"2024-02-05T06:25:08.070120Z","iopub.status.idle":"2024-02-05T06:25:08.074886Z","shell.execute_reply.started":"2024-02-05T06:25:08.070081Z","shell.execute_reply":"2024-02-05T06:25:08.073931Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def load_corpus(files, verbose=False):\n    if verbose:\n        print(f\"Loading files {files}\")\n\n    # reader = SimpleDirectoryReader(input_files=files)\n    reader = SimpleDirectoryReader(input_dir=files)\n    docs = reader.load_data()\n    if verbose:\n        print(f\"Loaded {len(docs)} docs\")\n\n    parser = SentenceSplitter()\n    nodes = parser.get_nodes_from_documents(docs, show_progress=verbose)\n\n    if verbose:\n        print(f\"Parsed {len(nodes)} nodes\")\n\n    return nodes","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:25:09.117232Z","iopub.execute_input":"2024-02-05T06:25:09.118121Z","iopub.status.idle":"2024-02-05T06:25:09.123917Z","shell.execute_reply.started":"2024-02-05T06:25:09.118085Z","shell.execute_reply":"2024-02-05T06:25:09.122916Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_nodes = load_corpus(TRAIN_FILES, verbose=True)\nval_nodes = load_corpus(VAL_FILES, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:25:10.468254Z","iopub.execute_input":"2024-02-05T06:25:10.468642Z","iopub.status.idle":"2024-02-05T06:25:10.892766Z","shell.execute_reply.started":"2024-02-05T06:25:10.468611Z","shell.execute_reply":"2024-02-05T06:25:10.891890Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Loading files /kaggle/input/aws-case-study-data-with-splits/data_samples/train/input/\nLoaded 12 docs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Parsing nodes:   0%|          | 0/12 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19398c7b305d4a0c81aaeb7ea9ea10ee"}},"metadata":{}},{"name":"stdout","text":"Parsed 32 nodes\nLoading files /kaggle/input/aws-case-study-data-with-splits/data_samples/val/input/\nLoaded 8 docs\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Parsing nodes:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8b78247c4de43079f7bc79d8f577806"}},"metadata":{}},{"name":"stdout","text":"Parsed 22 nodes\n","output_type":"stream"}]},{"cell_type":"code","source":"# !rm val_dataset.json train_dataset.json","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:25:16.720143Z","iopub.execute_input":"2024-02-05T06:25:16.720890Z","iopub.status.idle":"2024-02-05T06:25:16.724505Z","shell.execute_reply.started":"2024-02-05T06:25:16.720858Z","shell.execute_reply":"2024-02-05T06:25:16.723618Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from llama_index.finetuning import generate_qa_embedding_pairs, EmbeddingQAFinetuneDataset\n\ntrain_dataset = generate_qa_embedding_pairs(train_nodes, llm=llm, num_questions_per_chunk=2)\nval_dataset = generate_qa_embedding_pairs(val_nodes, llm=llm, num_questions_per_chunk=2)\n\ntrain_dataset.save_json(\"train_dataset.json\")\nval_dataset.save_json(\"val_dataset.json\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:25:40.181454Z","iopub.execute_input":"2024-02-05T06:25:40.182407Z","iopub.status.idle":"2024-02-05T06:30:09.566650Z","shell.execute_reply.started":"2024-02-05T06:25:40.182373Z","shell.execute_reply":"2024-02-05T06:30:09.565719Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"  0%|          | 0/32 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  3%|▎         | 1/32 [00:05<02:42,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  6%|▋         | 2/32 [00:10<02:45,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  9%|▉         | 3/32 [00:16<02:39,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 12%|█▎        | 4/32 [00:21<02:26,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 16%|█▌        | 5/32 [00:27<02:31,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 19%|█▉        | 6/32 [00:31<02:07,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 22%|██▏       | 7/32 [00:35<01:58,  4.75s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 25%|██▌       | 8/32 [00:40<01:52,  4.71s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 28%|██▊       | 9/32 [00:44<01:44,  4.53s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 31%|███▏      | 10/32 [00:49<01:46,  4.82s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 34%|███▍      | 11/32 [00:55<01:44,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 38%|███▊      | 12/32 [01:00<01:40,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 41%|████      | 13/32 [01:05<01:38,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 44%|████▍     | 14/32 [01:11<01:37,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 47%|████▋     | 15/32 [01:16<01:29,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 50%|█████     | 16/32 [01:20<01:19,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 53%|█████▎    | 17/32 [01:25<01:11,  4.74s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 56%|█████▋    | 18/32 [01:29<01:04,  4.63s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 59%|█████▉    | 19/32 [01:35<01:07,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 62%|██████▎   | 20/32 [01:40<00:58,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 66%|██████▌   | 21/32 [01:44<00:52,  4.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 69%|██████▉   | 22/32 [01:49<00:49,  4.96s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 72%|███████▏  | 23/32 [01:54<00:44,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 75%|███████▌  | 24/32 [02:00<00:40,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 78%|███████▊  | 25/32 [02:05<00:35,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 81%|████████▏ | 26/32 [02:10<00:30,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 84%|████████▍ | 27/32 [02:15<00:25,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 88%|████████▊ | 28/32 [02:19<00:19,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 91%|█████████ | 29/32 [02:24<00:14,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 94%|█████████▍| 30/32 [02:28<00:08,  4.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 97%|█████████▋| 31/32 [02:33<00:04,  4.64s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n100%|██████████| 32/32 [02:36<00:00,  4.90s/it]\n  0%|          | 0/22 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  5%|▍         | 1/22 [00:04<01:31,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n  9%|▉         | 2/22 [00:09<01:40,  5.04s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 14%|█▎        | 3/22 [00:15<01:39,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 18%|█▊        | 4/22 [00:21<01:39,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 23%|██▎       | 5/22 [00:27<01:39,  5.85s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 27%|██▋       | 6/22 [00:31<01:22,  5.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 32%|███▏      | 7/22 [00:37<01:23,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 36%|███▋      | 8/22 [00:42<01:15,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 41%|████      | 9/22 [00:47<01:07,  5.17s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 45%|████▌     | 10/22 [00:52<00:59,  4.94s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 50%|█████     | 11/22 [00:55<00:49,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 55%|█████▍    | 12/22 [00:59<00:43,  4.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 59%|█████▉    | 13/22 [01:04<00:40,  4.50s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 64%|██████▎   | 14/22 [01:08<00:34,  4.28s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 68%|██████▊   | 15/22 [01:11<00:28,  4.02s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 73%|███████▎  | 16/22 [01:17<00:27,  4.58s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 77%|███████▋  | 17/22 [01:21<00:22,  4.51s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 82%|████████▏ | 18/22 [01:27<00:19,  4.91s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 86%|████████▋ | 19/22 [01:35<00:17,  5.67s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 91%|█████████ | 20/22 [01:40<00:11,  5.68s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n 95%|█████████▌| 21/22 [01:46<00:05,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n100%|██████████| 22/22 [01:52<00:00,  5.11s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# ??generate_qa_embedding_pairs","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:30:09.569774Z","iopub.execute_input":"2024-02-05T06:30:09.570060Z","iopub.status.idle":"2024-02-05T06:30:09.574204Z","shell.execute_reply.started":"2024-02-05T06:30:09.570036Z","shell.execute_reply":"2024-02-05T06:30:09.573313Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# len(train_dataset.queries.keys())","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:30:09.575380Z","iopub.execute_input":"2024-02-05T06:30:09.575659Z","iopub.status.idle":"2024-02-05T06:30:09.585701Z","shell.execute_reply.started":"2024-02-05T06:30:09.575636Z","shell.execute_reply":"2024-02-05T06:30:09.584964Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### 2. Finetune","metadata":{}},{"cell_type":"code","source":"# !ls -lrt /kaggle/input/questions-w-docs","metadata":{"execution":{"iopub.status.busy":"2024-02-05T06:30:09.588197Z","iopub.execute_input":"2024-02-05T06:30:09.588461Z","iopub.status.idle":"2024-02-05T06:30:09.597872Z","shell.execute_reply.started":"2024-02-05T06:30:09.588438Z","shell.execute_reply":"2024-02-05T06:30:09.597136Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_dataset = EmbeddingQAFinetuneDataset.from_json(\"/kaggle/input/questions-w-docs/train_dataset.json\")\nval_dataset = EmbeddingQAFinetuneDataset.from_json(\"/kaggle/input/questions-w-docs/val_dataset.json\")","metadata":{"execution":{"iopub.status.busy":"2024-02-05T08:45:27.486070Z","iopub.execute_input":"2024-02-05T08:45:27.486808Z","iopub.status.idle":"2024-02-05T08:45:27.605987Z","shell.execute_reply.started":"2024-02-05T08:45:27.486772Z","shell.execute_reply":"2024-02-05T08:45:27.604973Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"base_embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n\nadapter_model = TwoLayerNN(\n    1024,  # input dimension\n    2048,  # hidden dimension\n    1024,  # output dimension\n    bias=True,\n    add_residual=True,\n)\n\nfinetune_engine_1 = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path=\"model_output_test\",\n    epochs=4,\n    verbose=True,\n    batch_size = 16\n)\n\nfinetune_engine_2 = EmbeddingAdapterFinetuneEngine(\n    train_dataset,\n    base_embed_model,\n    model_output_path=\"model25_output_test\",\n#     model_checkpoint_path=\"model10_ck\",\n    adapter_model=adapter_model,\n    epochs=10,\n    # bias=True,\n    verbose=True,\n    batch_size=32\n#     optimizer_class=torch.optim.AdamW,\n#     optimizer_params={\"lr\": 0.1}\n)\n\n# finetune_engine_3 = EmbeddingAdapterFinetuneEngine(\n#     train_dataset,\n#     base_embed_model,\n#     model_output_path=\"model50_output_test\",\n#     model_checkpoint_path=\"model50_ck\",\n#     adapter_model=adapter_model,\n#     epochs=50,\n#     # bias=True,\n#     verbose=True,\n#     batch_size=32\n# #     optimizer_class=torch.optim.AdamW,\n# #     optimizer_params={\"lr\": 0.1}\n# )","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:05:10.492948Z","iopub.execute_input":"2024-02-05T09:05:10.493326Z","iopub.status.idle":"2024-02-05T09:05:12.136568Z","shell.execute_reply.started":"2024-02-05T09:05:10.493296Z","shell.execute_reply":"2024-02-05T09:05:12.135729Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# finetune_engine_1.finetune()\nfinetune_engine_2.finetune()\n# finetune_engine_3.finetune()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:05:12.138105Z","iopub.execute_input":"2024-02-05T09:05:12.138395Z","iopub.status.idle":"2024-02-05T09:40:05.377880Z","shell.execute_reply.started":"2024-02-05T09:05:12.138369Z","shell.execute_reply":"2024-02-05T09:40:05.376713Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"\u001b[1;3;34m> Prepared optimizer, scheduler, and loss model.\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70c49fce3bb4180819163a71fc9bc44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec925bb79774c2ebb0c5d13536418ba"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 0] Current loss: 1.5551069974899292\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6143423318862915\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.600243330001831\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.376318097114563\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 2.7124955654144287\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 3.7398133277893066\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 2.7081539630889893\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.67755126953125\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 2.5075714588165283\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.520779013633728\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.4745124578475952\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.4212514162063599\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.4936225414276123\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6054432392120361\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.3321013450622559\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.696689248085022\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.4604642391204834\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6837546825408936\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.62134850025177\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.607418179512024\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6276984214782715\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5670392513275146\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.8188997507095337\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5377837419509888\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.3776898384094238\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5556700229644775\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.594031572341919\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 2.6039841175079346\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.453647494316101\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.668897271156311\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.514203429222107\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.382359266281128\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5203601121902466\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.319230556488037\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5436826944351196\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 3.2170748710632324\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6686781644821167\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6080920696258545\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.3374698162078857\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.408281922340393\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.44490647315979\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5626451969146729\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5091484785079956\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.5710337162017822\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.6299327611923218\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.2316821813583374\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.3495503664016724\n\u001b[0m\u001b[1;3;34m> [Epoch 0] Current loss: 1.2481348514556885\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa3e7bba87145afb9a6e9e6c4f7c3c4"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 1] Current loss: 1.555084466934204\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.614312767982483\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6002274751663208\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.376284122467041\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 2.712496519088745\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 3.739828109741211\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 2.7081410884857178\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6775016784667969\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 2.5075290203094482\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5207425355911255\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.4744760990142822\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.4211986064910889\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.4935972690582275\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6053717136383057\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.3320499658584595\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6966397762298584\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.460390567779541\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6836769580841064\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6212644577026367\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6073485612869263\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6275959014892578\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.566899061203003\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.8187129497528076\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5377097129821777\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.3775670528411865\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5555483102798462\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5939358472824097\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 2.6039326190948486\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.4533910751342773\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6687167882919312\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5140374898910522\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.382156491279602\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5199763774871826\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.3189765214920044\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5434482097625732\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 3.217189311981201\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6682653427124023\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6078037023544312\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.337212324142456\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.4079322814941406\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.444656252861023\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5623265504837036\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5088192224502563\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.5706762075424194\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.6295684576034546\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.2313328981399536\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.3492741584777832\n\u001b[0m\u001b[1;3;34m> [Epoch 1] Current loss: 1.2480403184890747\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1714330475124e8982ffec1566cb17ab"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 2] Current loss: 1.5545859336853027\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6138211488723755\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5999623537063599\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3757575750350952\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 2.7123844623565674\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 3.740010976791382\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 2.7076258659362793\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6768544912338257\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 2.5070371627807617\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5202854871749878\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4739975929260254\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4206823110580444\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4932674169540405\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6045936346054077\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3315849304199219\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6961250305175781\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4597439765930176\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6830250024795532\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6206268072128296\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6068202257156372\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6268073320388794\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5658073425292969\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.8174647092819214\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5371454954147339\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3767637014389038\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5548248291015625\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5933822393417358\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 2.6034293174743652\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.452052354812622\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6677329540252686\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5131362676620483\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.381231665611267\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5181320905685425\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3178147077560425\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.542384386062622\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 3.217660427093506\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6665043830871582\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.60654616355896\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3361246585845947\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4065637588500977\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.4436743259429932\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5611176490783691\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5075949430465698\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.5693670511245728\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.6282411813735962\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.2301186323165894\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.3483242988586426\n\u001b[0m\u001b[1;3;34m> [Epoch 2] Current loss: 1.247541069984436\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d917e11d2a14d138fc6c174563f2c46"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 3] Current loss: 1.552889347076416\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6122187376022339\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5991450548171997\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.3740875720977783\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 2.712041139602661\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 3.740683078765869\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 2.7059829235076904\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6749123334884644\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 2.505612850189209\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5189266204833984\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4725661277770996\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4192454814910889\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4922975301742554\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6024047136306763\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.330300211906433\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.694706916809082\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4579744338989258\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6812822818756104\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6189699172973633\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6054641008377075\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6247525215148926\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5629414319992065\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.8142949342727661\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5357071161270142\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.3747246265411377\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5530561208724976\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5920699834823608\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 2.6022586822509766\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4488648176193237\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6653854846954346\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5109902620315552\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.3791452646255493\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5139129161834717\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.3151965141296387\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5399876832962036\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 3.2187302112579346\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6626707315444946\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6037988662719727\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.3337796926498413\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.4036785364151\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.441601276397705\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.558627724647522\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5050938129425049\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.5667040348052979\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.6255581378936768\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.227691411972046\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.346455454826355\n\u001b[0m\u001b[1;3;34m> [Epoch 3] Current loss: 1.246596336364746\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f82793520b3419abe15e5f04beb82db"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 4] Current loss: 1.5495561361312866\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6091362237930298\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5976450443267822\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3709166049957275\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 2.7115132808685303\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 3.7421648502349854\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 2.702967643737793\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6713755130767822\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 2.5030596256256104\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5164488554000854\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.4699702262878418\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.4167317152023315\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.490563154220581\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5986077785491943\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3280714750289917\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6922646760940552\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.4549071788787842\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.678320288658142\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6161803007125854\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6032624244689941\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6213209629058838\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5581715106964111\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.809118628501892\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5333846807479858\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3713754415512085\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.550243616104126\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5900068283081055\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 2.600471019744873\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.4439055919647217\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6616963148117065\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5076531171798706\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3760067224502563\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.507659912109375\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3113170862197876\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.536432147026062\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 3.22031831741333\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.657231092453003\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5998551845550537\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.330445408821106\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.399643898010254\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.4386999607086182\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5551854372024536\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.5016975402832031\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.563134789466858\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.6219910383224487\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.2244913578033447\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.3440526723861694\n\u001b[0m\u001b[1;3;34m> [Epoch 4] Current loss: 1.2452938556671143\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f196808394d412b9d0a325628143d99"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 5] Current loss: 1.5453168153762817\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6052747964859009\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.595844030380249\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3670289516448975\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 2.7108798027038574\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 3.744094133377075\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 2.699138879776001\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6672817468643188\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 2.500040292739868\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.513505220413208\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.4669177532196045\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.41384756565094\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.488550066947937\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.594407081604004\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3255531787872314\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6895023584365845\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.4514750242233276\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.675038456916809\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6131088733673096\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6009418964385986\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6176080703735352\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.553087592124939\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.8036292791366577\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5309406518936157\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3677922487258911\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5473108291625977\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5878534317016602\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 2.5984182357788086\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.4388302564620972\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6578996181488037\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5042225122451782\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3728575706481934\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5014293193817139\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.307483434677124\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5329023599624634\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 3.221858263015747\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6519136428833008\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5960198640823364\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3271753787994385\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3957204818725586\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.4358595609664917\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.5518594980239868\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.4984380006790161\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.559719204902649\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.6186381578445435\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.2214380502700806\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.3417856693267822\n\u001b[0m\u001b[1;3;34m> [Epoch 5] Current loss: 1.2438194751739502\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282e304a890747d1af09f172136beee3"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 6] Current loss: 1.5413233041763306\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6016716957092285\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5941985845565796\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3634270429611206\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 2.7102060317993164\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 3.7459397315979004\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 2.695563793182373\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6636197566986084\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 2.4973089694976807\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5107969045639038\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4641484022140503\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.411241054534912\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4866939783096313\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5906717777252197\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3232759237289429\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6870143413543701\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4483715295791626\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6720905303955078\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.610370397567749\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5989271402359009\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6142733097076416\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5485669374465942\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.7987512350082397\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5288059711456299\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3645890951156616\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5447701215744019\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5859429836273193\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 2.5963587760925293\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4343953132629395\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6545637845993042\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5012236833572388\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.370151400566101\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4960838556289673\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3042113780975342\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5298759937286377\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 3.223146677017212\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6473809480667114\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.592767357826233\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3243902921676636\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3923877477645874\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4334430694580078\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5490469932556152\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.4956912994384766\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.5568463802337646\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.6158556938171387\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.2188842296600342\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.3399189710617065\n\u001b[0m\u001b[1;3;34m> [Epoch 6] Current loss: 1.2423951625823975\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f88017adbe849e59f1649b3701ea7cc"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 7] Current loss: 1.5380022525787354\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5986971855163574\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5928592681884766\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.360467553138733\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 2.7095625400543213\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 3.7475051879882812\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 2.6925735473632812\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6606953144073486\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 2.495100498199463\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5085843801498413\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4619048833847046\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4091418981552124\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4851795434951782\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5876985788345337\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.32145094871521\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6850159168243408\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4458694458007812\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6697205305099487\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6081798076629639\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5973702669143677\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6115970611572266\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5449635982513428\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.7948793172836304\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5271421670913696\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.3620327711105347\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5427902936935425\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5844477415084839\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 2.5946297645568848\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4309139251708984\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6519402265548706\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4988726377487183\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.3680592775344849\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4919626712799072\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.3017024993896484\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5275444984436035\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 3.224120616912842\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6439100503921509\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.590283751487732\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.322262167930603\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.3898460865020752\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4315987825393677\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.546912431716919\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.4936115741729736\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.5546817779541016\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.6137721538543701\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.2169721126556396\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.3385409116744995\n\u001b[0m\u001b[1;3;34m> [Epoch 7] Current loss: 1.241212248802185\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7045349a2293446fa74597b036801f3c"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 8] Current loss: 1.5355300903320312\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5965023040771484\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5918865203857422\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.3582987785339355\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 2.709029197692871\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 3.74870228767395\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 2.6903669834136963\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6586024761199951\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 2.493504524230957\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5069812536239624\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.460289478302002\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4076422452926636\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4840947389602661\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.585601806640625\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.3201580047607422\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6835993528366089\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4440973997116089\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6680471897125244\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6066433191299438\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.596306324005127\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6097240447998047\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5424525737762451\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.792198896408081\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5260041952133179\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.360267162322998\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.541445255279541\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5834298133850098\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 2.5934462547302246\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4285557270050049\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6501635313034058\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.497292399406433\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.3666696548461914\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4892383813858032\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.3000555038452148\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5260114669799805\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 3.224756956100464\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.641649842262268\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5886727571487427\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.320888638496399\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.3882131576538086\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4304171800613403\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5455509424209595\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.4922945499420166\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.5533229112625122\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.6124705076217651\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.2157868146896362\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.337697148323059\n\u001b[0m\u001b[1;3;34m> [Epoch 8] Current loss: 1.2404123544692993\n\u001b[0m","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/48 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99c7c1e7507441a96c5a1472717a26b"}},"metadata":{}},{"name":"stdout","text":"\u001b[1;3;34m> [Epoch 9] Current loss: 1.5340189933776855\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5951746702194214\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.59130859375\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3570085763931274\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 2.7086639404296875\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 3.7494163513183594\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 2.6890335083007812\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.657394289970398\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 2.492574691772461\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5060539245605469\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4593616724014282\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4067885875701904\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4834771156311035\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5844342708587646\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3194345235824585\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.682813048362732\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4431240558624268\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6671358346939087\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6058175563812256\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5957435369491577\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6087251901626587\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5411324501037598\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.7908068895339966\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5254155397415161\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3593672513961792\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5407708883285522\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5829159021377563\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 2.592817544937134\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4274119138717651\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6493079662322998\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4965420961380005\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3660227060317993\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4879944324493408\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.2993136644363403\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5253287553787231\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 3.225024938583374\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6406738758087158\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5879844427108765\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3203130960464478\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.3875398635864258\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4299381971359253\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5450036525726318\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.4917786121368408\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.5528059005737305\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.6119807958602905\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.215358853340149\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.337401032447815\n\u001b[0m\u001b[1;3;34m> [Epoch 9] Current loss: 1.24008047580719\n\u001b[0m\u001b[1;3;34m> Finished training, saving to voyage_model25_output_test\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# base_embed_model = None\n# finetune_engine_1 = None\n# finetune_engine_2 = None\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:43:42.197954Z","iopub.execute_input":"2024-02-05T09:43:42.198344Z","iopub.status.idle":"2024-02-05T09:43:42.203721Z","shell.execute_reply.started":"2024-02-05T09:43:42.198313Z","shell.execute_reply":"2024-02-05T09:43:42.202540Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"!zip -r emb_ft_large_saved.zip voyage_model_output_test voyage_model25_output_test\n# !du -sh ./voyage_model25_output_test/","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:49:27.870003Z","iopub.execute_input":"2024-02-05T09:49:27.870407Z","iopub.status.idle":"2024-02-05T09:49:30.209628Z","shell.execute_reply.started":"2024-02-05T09:49:27.870371Z","shell.execute_reply":"2024-02-05T09:49:30.208461Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"updating: voyage_model_output_test/ (stored 0%)\nupdating: voyage_model25_output_test/ (stored 0%)\n  adding: voyage_model_output_test/pytorch_model.bin (deflated 7%)\n  adding: voyage_model_output_test/config.json (deflated 29%)\n  adding: voyage_model25_output_test/pytorch_model.bin (deflated 8%)\n  adding: voyage_model25_output_test/config.json (deflated 32%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using ft embeddings for responding","metadata":{}},{"cell_type":"code","source":"val_documents = SimpleDirectoryReader(\"/kaggle/input/aws-case-study-data-with-splits/data/val/input\").load_data()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:40:46.319929Z","iopub.execute_input":"2024-02-05T09:40:46.320327Z","iopub.status.idle":"2024-02-05T09:40:46.695415Z","shell.execute_reply.started":"2024-02-05T09:40:46.320294Z","shell.execute_reply":"2024-02-05T09:40:46.694690Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# # sampling questions\n# def _sample_question():\n#     import json\n#     import random\n#     with open(\"/kaggle/input/questions-w-docs/val_dataset.json\", \"r\") as f:\n#         load = json.load(f)\n#     queries = load[\"queries\"]\n#     hash_key, question = random.choice(list(queries.items()))\n#     key_to_relevant_doc = load[\"relevant_docs\"][hash_key]\n#     relevant_doc = load[\"corpus\"][key_to_relevant_doc[0]]\n#     print(question)\n#     print(relevant_doc)\n# _sample_question()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:40:48.392665Z","iopub.execute_input":"2024-02-05T09:40:48.393485Z","iopub.status.idle":"2024-02-05T09:40:48.397465Z","shell.execute_reply.started":"2024-02-05T09:40:48.393451Z","shell.execute_reply":"2024-02-05T09:40:48.396597Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"import json\nimport random\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:40:48.865706Z","iopub.execute_input":"2024-02-05T09:40:48.866613Z","iopub.status.idle":"2024-02-05T09:40:48.870377Z","shell.execute_reply.started":"2024-02-05T09:40:48.866578Z","shell.execute_reply":"2024-02-05T09:40:48.869532Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# function to randomly sample a question from val_set, and respond to them using all embedding versions\n\ndef show_generation():\n        # sampling a random question & relevant doc from val set\n        with open(\"/kaggle/input/questions-w-docs/val_dataset.json\", \"r\") as f:\n            load = json.load(f)\n        queries = load[\"queries\"]\n        hash_key, question = random.choice(list(queries.items()))\n        key_to_relevant_doc = load[\"relevant_docs\"][hash_key]\n        relevant_doc = load[\"corpus\"][key_to_relevant_doc[0]]\n        print(\"Question sampled is:: \", question)\n        print()\n        \n        # prepare embedding models\n        base_embed_model = resolve_embed_model(\"local:BAAI/bge-large-en-v1.5\")\n        embed_model_1 = LinearAdapterEmbeddingModel(base_embed_model,\n                                                    \"/kaggle/input/ft-embeddings/model_output_test\")\n        embed_model_2_2layer = AdapterEmbeddingModel(base_embed_model,\n                                                     \"/kaggle/input/ft-embeddings/model10_output_test\",\n                                                     TwoLayerNN)\n        embed_model_3_2layer = AdapterEmbeddingModel(base_embed_model,\n                                                     \"/kaggle/input/ft-embeddings/model50_output_test\",\n                                                     TwoLayerNN)\n        emb_models = [base_embed_model, embed_model_1, embed_model_2_2layer, embed_model_3_2layer]\n\n        # using every embedding model generate response\n        for emb in emb_models:\n            service_context = ServiceContext.from_defaults(llm=llm, embed_model=emb)\n            vector_index = VectorStoreIndex.from_documents(val_documents, service_context=service_context)\n    #         summary_index = SummaryIndex.from_documents(documents, service_context=service_context)\n\n            query_engine = vector_index.as_query_engine(response_mode=\"compact\")\n            start = time.time()\n            response = query_engine.query(question)\n            print(f\"Time taken to generate response:: {time.time()-start}\")\n            print(f\"Generated response using embedding: {emb} ::\")\n            display_response(response)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:43:51.364522Z","iopub.execute_input":"2024-02-05T09:43:51.364906Z","iopub.status.idle":"2024-02-05T09:43:51.374543Z","shell.execute_reply.started":"2024-02-05T09:43:51.364874Z","shell.execute_reply":"2024-02-05T09:43:51.373589Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"show_generation()","metadata":{"execution":{"iopub.status.busy":"2024-02-05T09:43:56.170783Z","iopub.execute_input":"2024-02-05T09:43:56.171925Z","iopub.status.idle":"2024-02-05T09:43:57.300018Z","shell.execute_reply.started":"2024-02-05T09:43:56.171883Z","shell.execute_reply":"2024-02-05T09:43:57.298624Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Question sampled is::  What was the main challenge faced by Zalando with its previous image management solution, and how did Amazon CloudFront help address this challenge?\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mshow_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[74], line 15\u001b[0m, in \u001b[0;36mshow_generation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# prepare embedding models\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m base_embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mresolve_embed_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal:BAAI/bge-large-en-v1.5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m embed_model_1 \u001b[38;5;241m=\u001b[39m LinearAdapterEmbeddingModel(base_embed_model,\n\u001b[1;32m     17\u001b[0m                                             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/ft-embeddings/model_output_test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m embed_model_2_2layer \u001b[38;5;241m=\u001b[39m AdapterEmbeddingModel(base_embed_model,\n\u001b[1;32m     19\u001b[0m                                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/ft-embeddings/model10_output_test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m                                              TwoLayerNN)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/embeddings/utils.py:84\u001b[0m, in \u001b[0;36mresolve_embed_model\u001b[0;34m(embed_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m InstructorEmbedding(\n\u001b[1;32m     81\u001b[0m             model_name\u001b[38;5;241m=\u001b[39mmodel_name, cache_folder\u001b[38;5;241m=\u001b[39mcache_folder\n\u001b[1;32m     82\u001b[0m         )\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m         embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LCEmbeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embed_model, LCEmbeddings):\n\u001b[1;32m     89\u001b[0m     embed_model \u001b[38;5;241m=\u001b[39m LangchainEmbedding(embed_model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/llama_index/embeddings/huggingface.py:87\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Extract model_name from model\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Use tokenizer_name with AutoTokenizer\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     tokenizer_name \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         model_name \u001b[38;5;129;01mor\u001b[39;00m tokenizer_name \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_HUGGINGFACE_EMBEDDING_MODEL\n\u001b[1;32m     92\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:2595\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2591\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2592\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2593\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2594\u001b[0m         )\n\u001b[0;32m-> 2595\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 120.00 MiB. GPU 1 has a total capacty of 14.75 GiB of which 3.06 MiB is free. Process 2543 has 14.74 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 175.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 120.00 MiB. GPU 1 has a total capacty of 14.75 GiB of which 3.06 MiB is free. Process 2543 has 14.74 GiB memory in use. Of the allocated memory 14.44 GiB is allocated by PyTorch, and 175.14 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from llama_index.embeddings import LinearAdapterEmbeddingModel\nbase_embed_model = resolve_embed_model(\"local:BAAI/bge-small-en\")\nembed_model_1 = LinearAdapterEmbeddingModel(base_embed_model, \"/kaggle/input/ft-embeddings/model_output_test\")\n\nembed_model_2_2layer = AdapterEmbeddingModel(\n    base_embed_model,\n    \"/kaggle/input/ft-embeddings/model10_output_test\",\n    TwoLayerNN,\n)\n\nembed_model_3_2layer = AdapterEmbeddingModel(\n    base_embed_model,\n    \"/kaggle/input/ft-embeddings/model50_output_test\",\n    TwoLayerNN,\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-05T07:48:12.304922Z","iopub.execute_input":"2024-02-05T07:48:12.305829Z","iopub.status.idle":"2024-02-05T07:48:12.915665Z","shell.execute_reply.started":"2024-02-05T07:48:12.305795Z","shell.execute_reply":"2024-02-05T07:48:12.914786Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\")\nvector_index = VectorStoreIndex.from_documents(documents, service_context=service_context)\nsummary_index = SummaryIndex.from_documents(documents, service_context=service_context)","metadata":{},"execution_count":null,"outputs":[]}]}